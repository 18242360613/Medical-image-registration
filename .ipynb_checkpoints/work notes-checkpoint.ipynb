{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal in brief ###\n",
    "\n",
    "Registrating brain images (2D/3D) with NNs. \n",
    "\n",
    "### Survey of architectures / Possible ideas###\n",
    "Most recent work are all using a \"bottleneck\" structure somewhere.  \n",
    "* <span style =\"color: blue\"> Add a VAE infront for generalization purpose? It also provides a way to compress the data</span>\n",
    "\n",
    "Some work utilizes `spatial transformation netwrok` to enforce the net learns spatial transforms.\n",
    "* <span style =\"color: blue\"> Use certain loss (e.g. gradient related, if so, apply a gaussian filter first?) to regularize? </span>\n",
    "\n",
    "\n",
    "### Other Ideas ###\n",
    "* A two stage registration. First stage learns the spatial relationship, second stage learns to draw the intensity better.        \n",
    "   <span style = 'color: blue'> How to decide if the spatial relationship is learned adequetly? using certain kinds of contour   mask?\n",
    "\n",
    "### Questions during actual implementation###\n",
    "* In the code `spatialdeformer.py`, why \n",
    "```\n",
    "deformation = tf.reshape(deformation, shape = (batch_size, 2, -1) )\n",
    "```\n",
    "\n",
    "is not the same as \n",
    "\n",
    "```\n",
    "deformation = tf.reshape(deformation, (-1, output_height * output_width, 2))\n",
    "deformation = tf.transpose(deformation, (0, 2, 1))\n",
    "```\n",
    "\n",
    "The latter gives desired result, the former does not (Gradient seems to be very small for its update).\n",
    "\n",
    "<span style = 'color: blue'> The truth is the mechanism of `tf.reshape()`, it will start filling the tensor using digits beginning from the last dimension of the original tensor. The deformation is of shape (batch_size, height, width, 2). So in order to make the information stay in the same channel, you have to do it the latter way. Even though these weight are latent and learned during training.</span>\n",
    "\n",
    "* Stacking a STN infront of a SDN will produce a warning relating to converting a sparse tense into a dense one implicitly.\n",
    "(The out put of STN is of shape (?, height, width, ?), why the channel info is not seen?)\n",
    "\n",
    "* Is it necessary to do some preprocess like substract mean from trainning channel-wised?\n",
    "\n",
    "<span style = 'color: blue'> Does not seem to help much, there will be stripes at the border, and the loss curve does not look nice. Maybe other preprocess will help, check what do others do. </span>\n",
    "\n",
    "* **Warping small ventricles to large ones seems to be more difficult than the other way around**. \n",
    "\n",
    "<span style = 'color: blue'> This may because of the nature of your SDN which uses the warped grid to sample, if the moving image has a gap that is only 2 or 3 pixels wide, it is hard for many grid points to be squeezed in that gap. </span>\n",
    "\n",
    "<span style = 'color: blue'> Could let the same network to predict an \"inverse-warped\" image</span>\n",
    "\n",
    "* It is hard to learn local sturctures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused on losses###\n",
    "\n",
    "* Single loss: means_sqaure_logarithmic_error and KL-divergence seems to work good\n",
    "   \n",
    "     * Their scales on train. KL: (negative when using img directly, ~0.10 when flattened and normalized by sum)\n",
    "                              SobelLoss: ~0.08 \n",
    "                              MSE: ~0.08\n",
    "                              MSE_log: ~0.005   \n",
    "                              BCE: ~0.3\n",
    "                              TVS: ~0.02 with $\\alpha = 1.25$\n",
    "\n",
    "* Using perception loss extracted from other pretrained models.\n",
    "                              \n",
    "### Structure of the regress net ###\n",
    "\n",
    "* Choice of uppooling: upsampling or conv_transpose.\n",
    "\n",
    "* Adding paralelled SDN modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
